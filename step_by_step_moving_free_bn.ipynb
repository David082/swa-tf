{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from math import ceil\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "EPOCHS = 50\n",
    "INIT_LR = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "- load `train` and `test` subsets (CIFAR-10)\n",
    "- split `train` into `train` + `valid` (80/20%, stratified split on labels)\n",
    "\n",
    "- create data generators (with `keras.preprocessing.image.ImageDataGenerator`):\n",
    "    - one ImageDataGenerator with data augmentation (horizontal flips, random translations) for train set\n",
    "    - three ImageDataGenerator without data augmentation for train, valid and test subset \n",
    "        - why `train` ? : to fit Batch Norm statistics without augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading CIFAR10 dataset ...\n",
      "\tTRAIN - images (40000, 32, 32, 3) | float32  - labels (40000,) - int32\n",
      "\tVAL - images (10000, 32, 32, 3) | float32  - labels (10000,) - int32\n",
      "\tTEST - images (10000, 32, 32, 3) | float32  - labels (10000,) - int32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"... loading CIFAR10 dataset ...\")\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train,\n",
    "                                                  test_size=0.2,\n",
    "                                                  stratify=y_train,\n",
    "                                                  random_state=51)\n",
    "# cast samples and labels\n",
    "x_train = x_train.astype(np.float32) / 255.\n",
    "x_val = x_val.astype(np.float32) / 255.\n",
    "x_test = x_test.astype(np.float32) / 255.\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_val = y_val.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "\n",
    "print(\"\\tTRAIN - images {} | {}  - labels {} - {}\".format(x_train.shape, x_train.dtype, y_train.shape, y_train.dtype))\n",
    "print(\"\\tVAL - images {} | {}  - labels {} - {}\".format(x_val.shape, x_val.dtype, y_val.shape, y_val.dtype))\n",
    "print(\"\\tTEST - images {} | {}  - labels {} - {}\\n\".format(x_test.shape, x_test.dtype, y_test.shape, y_test.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_aug = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=5,\n",
    "                                                                height_shift_range=5,\n",
    "                                                                fill_mode='constant',\n",
    "                                                                cval=0.0,\n",
    "                                                                rotation_range=90.,\n",
    "                                                                horizontal_flip=True,\n",
    "                                                                vertical_flip=True)\n",
    "\n",
    "generator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "# python iterator object that yields augmented samples \n",
    "iterator_train_aug = generator_aug.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "\n",
    "# python iterators object that yields not augmented samples \n",
    "iterator_train = generator.flow(x_train, y_train, batch_size=BATCH_SIZE)\n",
    "iterator_valid = generator.flow(x_val, y_val, batch_size=BATCH_SIZE)\n",
    "iterator_test = generator.flow(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "\n",
    "steps_per_epoch_train = int(ceil(iterator_train.n/BATCH_SIZE))\n",
    "steps_per_epoch_val = int(ceil(iterator_valid.n/BATCH_SIZE))\n",
    "steps_per_epoch_test = int(ceil(iterator_test.n/BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label : 9\n",
      "x : (16, 32, 32, 3) | float32\n",
      "y : (16,) | int32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd58bbf978>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test iterator with data augmentation\n",
    "x, y = iterator_train_aug.next()\n",
    "\n",
    "i = 0 \n",
    "img = x[i]*255\n",
    "label = y[i]\n",
    "print(\"label : {}\".format(label))\n",
    "print(\"x : {} | {}\".format(x.shape, x.dtype))\n",
    "print(\"y : {} | {}\".format(y.shape, y.dtype))\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(img.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now data iterators are ready ! \n",
    "\n",
    "# Build a network with MovingFreeBatchNorm layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moving_free_batch_normalization import moving_free_batch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a TensorFlow session\n",
    "config = tf.ConfigProto()\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs: only placeholders (easy to use) !\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    # to images\n",
    "    batch_x = tf.placeholder(shape=[None, 32, 32, 3], dtype=tf.float32)\n",
    "    \n",
    "    # to feed labels\n",
    "    batch_y = tf.placeholder(shape=[None, ], dtype=tf.int64)\n",
    "    \n",
    "    # placeholders to set the learning phase (train vs inference)\n",
    "    # this controls the behavior of BN layers \n",
    "    is_training_bn = tf.placeholder(shape=[], dtype=tf.bool)\n",
    "    \n",
    "    # select which statistics (mean, variance) to use in BN layers during inference \n",
    "    # when is_training_bn is False !\n",
    "    use_moving_statistics = tf.placeholder(shape=[], dtype=tf.bool)\n",
    "    \n",
    "    # placeholder for learning rate for learning scheduling \n",
    "    learning_rate = tf.placeholder(shape=[], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network similar to a VGG11 \n",
    "\n",
    "with tf.name_scope('network'):\n",
    "    ######################### 32x32 features per feature maps #########################\n",
    "    x = tf.layers.conv2d(batch_x, filters=64, kernel_size=3, strides=1, use_bias=True,\n",
    "                         activation=tf.nn.relu, data_format='channels_last', padding='same')\n",
    "    x = moving_free_batch_norm(x, axis=-1, training=is_training_bn,\n",
    "                               use_moving_statistics=use_moving_statistics, momentum=0.99)\n",
    "    x = tf.layers.max_pooling2d(x, pool_size=2, strides=2, data_format='channels_last', padding='same')\n",
    "    ###################################################################################\n",
    "\n",
    "    \n",
    "    ######################### 16x16 features per feature maps #########################\n",
    "    x = tf.layers.conv2d(x, filters=128, kernel_size=3, strides=1, use_bias=True,\n",
    "                         activation=tf.nn.relu, data_format='channels_last', padding='same')\n",
    "    x = moving_free_batch_norm(x, axis=-1, training=is_training_bn,\n",
    "                               use_moving_statistics=use_moving_statistics, momentum=0.99)\n",
    "    x = tf.layers.max_pooling2d(x, pool_size=2, strides=2, data_format='channels_last', padding='same')\n",
    "    ###################################################################################\n",
    "    \n",
    "    ######################### 8x8 features per feature maps #########################\n",
    "    x = tf.layers.conv2d(x, filters=256, kernel_size=3, strides=1, use_bias=True,\n",
    "                         activation=tf.nn.relu, data_format='channels_last', padding='same')\n",
    "    x = moving_free_batch_norm(x, axis=-1, training=is_training_bn, use_moving_statistics=use_moving_statistics, momentum=0.99)\n",
    "    x = tf.layers.conv2d(x, filters=256, kernel_size=3, strides=1, use_bias=True,\n",
    "                         activation=tf.nn.relu, data_format='channels_last', padding='same')\n",
    "    x = moving_free_batch_norm(x, axis=-1, training=is_training_bn,\n",
    "                               use_moving_statistics=use_moving_statistics, momentum=0.99)\n",
    "    x = tf.layers.max_pooling2d(x, pool_size=2, strides=2, data_format='channels_last', padding='same')\n",
    "    ###################################################################################\n",
    "    \n",
    "    ######################### 4x4 features per feature maps #########################\n",
    "    x = tf.layers.conv2d(x, filters=512, kernel_size=3, strides=1, use_bias=True,\n",
    "                         activation=tf.nn.relu, data_format='channels_last', padding='same')\n",
    "    x = moving_free_batch_norm(x, axis=-1, training=is_training_bn, use_moving_statistics=use_moving_statistics, momentum=0.99)\n",
    "    x = tf.layers.conv2d(x, filters=512, kernel_size=3, strides=1, use_bias=True,\n",
    "                         activation=tf.nn.relu, data_format='channels_last', padding='same')\n",
    "    x = moving_free_batch_norm(x, axis=-1, training=is_training_bn,\n",
    "                               use_moving_statistics=use_moving_statistics, momentum=0.99)\n",
    "    x = tf.layers.max_pooling2d(x, pool_size=2, strides=2, data_format='channels_last', padding='same')\n",
    "    ###################################################################################\n",
    "    \n",
    "    ######################### 2x2 features per feature maps #########################\n",
    "    x = tf.layers.conv2d(x, filters=512, kernel_size=3, strides=1, use_bias=True,\n",
    "                         activation=tf.nn.relu, data_format='channels_last', padding='same')\n",
    "    x = moving_free_batch_norm(x, axis=-1, training=is_training_bn, use_moving_statistics=use_moving_statistics, momentum=0.99)\n",
    "    x = tf.layers.conv2d(x, filters=512, kernel_size=3, strides=1, use_bias=True,\n",
    "                         activation=tf.nn.relu, data_format='channels_last', padding='same')\n",
    "    x = moving_free_batch_norm(x, axis=-1, training=is_training_bn,\n",
    "                               use_moving_statistics=use_moving_statistics, momentum=0.99)\n",
    "    ###################################################################################\n",
    "    \n",
    "    ######################### Global Average Pooling #########################\n",
    "    x = tf.reduce_mean(x, axis=[1, 2]) \n",
    "    logits = tf.layers.dense(x, units=10, use_bias=True)\n",
    "    ###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 64) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d/bias:0' shape=(64,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization/gamma:0' shape=(64,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization/beta:0' shape=(64,) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 64, 128) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_1/bias:0' shape=(128,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_1/gamma:0' shape=(128,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_1/beta:0' shape=(128,) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 128, 256) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_2/bias:0' shape=(256,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_2/gamma:0' shape=(256,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_2/beta:0' shape=(256,) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 256, 256) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_3/bias:0' shape=(256,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_3/gamma:0' shape=(256,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_3/beta:0' shape=(256,) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_4/kernel:0' shape=(3, 3, 256, 512) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_4/bias:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_4/gamma:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_4/beta:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_5/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_5/bias:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_5/gamma:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_5/beta:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_6/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_6/bias:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_6/gamma:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_6/beta:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_7/kernel:0' shape=(3, 3, 512, 512) dtype=float32_ref>,\n",
      " <tf.Variable 'conv2d_7/bias:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_7/gamma:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Variable 'moving_free_batch_normalization_7/beta:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Variable 'dense/kernel:0' shape=(512, 10) dtype=float32_ref>,\n",
      " <tf.Variable 'dense/bias:0' shape=(10,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "model_vars = tf.trainable_variables()\n",
    "pprint(model_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Operation 'network/moving_free_batch_normalization/AssignMovingAvg' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization/AssignMovingAvg_1' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_1/AssignMovingAvg' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_1/AssignMovingAvg_1' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_2/AssignMovingAvg' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_2/AssignMovingAvg_1' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_3/AssignMovingAvg' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_3/AssignMovingAvg_1' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_4/AssignMovingAvg' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_4/AssignMovingAvg_1' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_5/AssignMovingAvg' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_5/AssignMovingAvg_1' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_6/AssignMovingAvg' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_6/AssignMovingAvg_1' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_7/AssignMovingAvg' type=AssignSub>,\n",
      " <tf.Operation 'network/moving_free_batch_normalization_7/AssignMovingAvg_1' type=AssignSub>]\n",
      "[<tf.Tensor 'network/moving_free_batch_normalization/AssignAdd:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_1/AssignAdd:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_2/AssignAdd:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_3/AssignAdd:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_4/AssignAdd:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_5/AssignAdd:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_6/AssignAdd:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_7/AssignAdd:0' shape=() dtype=float32_ref>]\n",
      "[<tf.Tensor 'network/moving_free_batch_normalization/Assign:0' shape=(64,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization/Assign_1:0' shape=(64,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization/Assign_2:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_1/Assign:0' shape=(128,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_1/Assign_1:0' shape=(128,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_1/Assign_2:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_2/Assign:0' shape=(256,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_2/Assign_1:0' shape=(256,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_2/Assign_2:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_3/Assign:0' shape=(256,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_3/Assign_1:0' shape=(256,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_3/Assign_2:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_4/Assign:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_4/Assign_1:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_4/Assign_2:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_5/Assign:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_5/Assign_1:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_5/Assign_2:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_6/Assign:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_6/Assign_1:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_6/Assign_2:0' shape=() dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_7/Assign:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_7/Assign_1:0' shape=(512,) dtype=float32_ref>,\n",
      " <tf.Tensor 'network/moving_free_batch_normalization_7/Assign_2:0' shape=() dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "# operations to update moving averages in batch norm layers\n",
    "# to run before updating weights ! (with tf.control_dependencies())\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "# these operations are new, specific to MovingFreeBatchNorm layers.\n",
    "# operations to updates (mean, variance, n) in batch norm layers \n",
    "update_bn_ops = tf.get_collection('UPDATE_BN_OPS')\n",
    "# operations to reset (mean, variance, n) to zero \n",
    "reset_bn_ops = tf.get_collection('RESET_BN_OPS')\n",
    "\n",
    "pprint(update_ops)\n",
    "pprint(update_bn_ops)\n",
    "pprint(reset_bn_ops)\n",
    "\n",
    "# group these operations\n",
    "update_ops = tf.group(*update_ops)\n",
    "update_bn_ops = tf.group(*update_bn_ops)\n",
    "reset_bn_ops = tf.group(*reset_bn_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add loss and accuracy \n",
    "with tf.name_scope('loss'):\n",
    "    loss_tf = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                                            labels=batch_y))\n",
    "    acc_tf = tf.reduce_mean(tf.cast(tf.equal(batch_y, tf.argmax(logits, axis=1)), dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization with SGD x Momentum\n",
    "with tf.name_scope('optimizer'):\n",
    "    opt = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "    \n",
    "    # even with MovingFreeBatchNorm we can update the moving statistics\n",
    "    with tf.control_dependencies([update_ops,]):\n",
    "        train_op = opt.minimize(loss_tf, var_list=model_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize all parameters \n",
    "global_init_op = tf.global_variables_initializer()\n",
    "sess.run(global_init_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant learning rate\n",
    "def get_learning_rate(step, epoch, steps_per_epoch_train):\n",
    "    return INIT_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch traninig \n",
    "- create a function to fit statistics with train subset\n",
    "- create a function to run inference on [train/]val/test subsets (with/without moving statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bn_statistics():\n",
    "    # reset new BN statistics\n",
    "    sess.run(reset_bn_ops)\n",
    "    \n",
    "    # when updating, set is_training_bn=True\n",
    "    feed_dict = {is_training_bn: True, use_moving_statistics: True}\n",
    "    for _ in range(steps_per_epoch_train):\n",
    "        # use sample from the train set, without data augmentation\n",
    "        x, y = iterator_train.next()\n",
    "        feed_dict[batch_x] = x\n",
    "        sess.run(update_bn_ops, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(iterator, with_moving_statistics=True):\n",
    "    \n",
    "    feed_dict = {is_training_bn: False,\n",
    "                 use_moving_statistics: with_moving_statistics}\n",
    "    \n",
    "    all_acc = []\n",
    "    all_loss = []\n",
    "    nb_steps = int(ceil(iterator.n/BATCH_SIZE))\n",
    "    \n",
    "    for _ in range(nb_steps):\n",
    "        x, y = iterator.next()\n",
    "        feed_dict[batch_x] = x\n",
    "        feed_dict[batch_y] = y\n",
    "        acc_v, loss_v = sess.run([acc_tf, loss_tf], feed_dict=feed_dict)\n",
    "        all_acc.append(acc_v)\n",
    "        all_loss.append(loss_v)\n",
    "    \n",
    "    return np.mean(all_acc), np.mean(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random model : acc=0.10410   loss=2.30254\n",
      "CPU times: user 19.6 s, sys: 7.76 s, total: 27.4 s\n",
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# test your random model ? \n",
    "acc, loss = inference(iterator_valid, with_moving_statistics=True)\n",
    "print(\"Random model : acc={:.5f}   loss={:.5f}\".format(acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random model : acc=0.09230   loss=2.53787\n",
      "CPU times: user 1min 33s, sys: 36 s, total: 2min 9s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# now fit batch norm statistics and make inference again \n",
    "fit_bn_statistics()\n",
    "acc, loss = inference(iterator_valid, with_moving_statistics=False)\n",
    "print(\"Random model : acc={:.5f}   loss={:.5f}\".format(acc, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN @ EPOCH 0 : acc=0.31610  loss=1.87514  in 448.770 s\n",
      "VALID @ EPOCH 0 : acc=0.40390   loss=1.64809  in 472.807 s\n",
      "VALID updated BN @ EPOCH 0 : acc=0.40760   loss=1.65106  in 596.536 s\n",
      "TRAIN @ EPOCH 1 : acc=0.40047  loss=1.66038  in 1043.320 s\n",
      "VALID @ EPOCH 1 : acc=0.43330   loss=1.62047  in 1067.253 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "step = 0\n",
    "start = time()\n",
    "\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "\n",
    "acc_val = []\n",
    "loss_val = []\n",
    "\n",
    "acc_val_bn = []\n",
    "loss_val_bn = []\n",
    "\n",
    "feed_dict_train = {is_training_bn: True, \n",
    "                   use_moving_statistics:True,}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    acc = []\n",
    "    loss = []\n",
    "    for _ in range(steps_per_epoch_train):\n",
    "        \n",
    "        # get samples from the train set, with data augmentation \n",
    "        x, y =  iterator_train_aug.next()\n",
    "        feed_dict_train[batch_x] = x\n",
    "        feed_dict_train[batch_y] = y\n",
    "        feed_dict_train[learning_rate] = get_learning_rate(step, epoch, steps_per_epoch_train)\n",
    "        \n",
    "        acc_v, loss_v, _ = sess.run([acc_tf, loss_tf, train_op], feed_dict=feed_dict_train)\n",
    "        acc.append(acc_v)\n",
    "        loss.append(loss_v)\n",
    "        step += 1\n",
    "        \n",
    "    acc = np.mean(acc)\n",
    "    loss = np.mean(loss)\n",
    "    acc_train.append((epoch,acc))\n",
    "    loss_train.append((epoch,loss))\n",
    "    print(\"TRAIN @ EPOCH {} : acc={:.5f}  loss={:.5f}  in {:.3f} s\".format(epoch, acc, loss,\n",
    "                                                                               time()-start))\n",
    "    \n",
    "    acc, loss = inference(iterator_valid, with_moving_statistics=True)\n",
    "    acc_val.append((epoch,acc))\n",
    "    loss_val.append((epoch,loss))\n",
    "    print(\"VALID @ EPOCH {} : acc={:.5f}   loss={:.5f}  in {:.3f} s\".format(epoch, acc, loss,\n",
    "                                                                            time()-start))\n",
    "\n",
    "    # now fit batch norm statistics and make inference again \n",
    "    fit_bn_statistics()\n",
    "    acc, loss = inference(iterator_valid, with_moving_statistics=False)\n",
    "    acc_val_bn.append((epoch,acc))\n",
    "    loss_val_bn.append((epoch,loss))\n",
    "    print(\"VALID updated BN @ EPOCH {} : acc={:.5f}   loss={:.5f}  in {:.3f} s\".format(epoch, acc,\n",
    "                                                                                       loss, time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
